{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cede9f0-70c7-45e3-9a3b-7c75f6a984e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-03 02:44:05,294] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import amplify\n",
    "import os\n",
    "\n",
    "# load the model\n",
    "MODEL = \"AMPLIFY_350M\"\n",
    "path = f\"../../../data/models/AMPLIFY/{MODEL}/\"\n",
    "config_path = os.path.join(path, \"config.yaml\")\n",
    "checkpoint_file = os.path.join(path, \"pytorch_model.pt\")\n",
    "\n",
    "model, tokenizer = amplify.AMPLIFY.load(checkpoint_file, config_path)\n",
    "predictor = amplify.inference.Predictor(model, tokenizer, device=\"cuda\")\n",
    "\n",
    "# !python ../scripts/convert_checkpoint_to_nemo.py \\\n",
    "#         --input_name_or_path {checkpoint_file} \\\n",
    "#         --output_path . \\\n",
    "#         --config_name_or_path {config_path}\\\n",
    "#         --hparams_file ../conf/megatron_amplify_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33c6fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1003 03:00:17.204801 401706 pinned_memory_manager.cc:277] \"Pinned memory pool is created at '0x205000000' with size 268435456\"\n",
      "I1003 03:00:17.204953 401706 cuda_memory_manager.cc:107] \"CUDA memory pool is created on device 0 with size 67108864\"\n",
      "I1003 03:00:17.209178 401706 server.cc:604] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I1003 03:00:17.209229 401706 server.cc:631] \n",
      "+---------+------+--------+\n",
      "| Backend | Path | Config |\n",
      "+---------+------+--------+\n",
      "+---------+------+--------+\n",
      "\n",
      "I1003 03:00:17.209243 401706 server.cc:674] \n",
      "+-------+---------+--------+\n",
      "| Model | Version | Status |\n",
      "+-------+---------+--------+\n",
      "+-------+---------+--------+\n",
      "\n",
      "I1003 03:00:17.243143 401706 metrics.cc:877] \"Collecting metrics for GPU 0: NVIDIA RTX 1000 Ada Generation Laptop GPU\"\n",
      "I1003 03:00:17.249444 401706 metrics.cc:770] \"Collecting CPU metrics\"\n",
      "I1003 03:00:17.250181 401706 tritonserver.cc:2598] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.48.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data parameters statistics trace  |\n",
      "|                                  | logging                                  |\n",
      "| model_repository_path[0]         | /root/.cache/pytriton/workspace_n424ehky |\n",
      "|                                  | /model-store                             |\n",
      "| model_control_mode               | MODE_EXPLICIT                            |\n",
      "| startup_models_0                 | *                                        |\n",
      "| strict_model_config              | 0                                        |\n",
      "| model_config_name                |                                          |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "| cache_enabled                    | 0                                        |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1003 03:00:17.262257 401706 grpc_server.cc:2463] \"Started GRPCInferenceService at 0.0.0.0:8001\"\n",
      "I1003 03:00:17.262494 401706 http_server.cc:4692] \"Started HTTPService at 0.0.0.0:9999\"\n",
      "I1003 03:00:17.309733 401706 http_server.cc:362] \"Started Metrics Service at 0.0.0.0:8002\"\n",
      "I1003 03:00:18.062244 401706 model_lifecycle.cc:472] \"loading: AMPLIFY:1\"\n",
      "W1003 03:00:18.254000 401706 metrics.cc:631] \"Unable to get power limit for GPU 0. Status:Success, value:0.000000\"\n",
      "W1003 03:00:19.260547 401706 metrics.cc:631] \"Unable to get power limit for GPU 0. Status:Success, value:0.000000\"\n",
      "I1003 03:00:20.011776 401706 python_be.cc:1912] \"TRITONBACKEND_ModelInstanceInitialize: AMPLIFY_0_0 (CPU device 0)\"\n",
      "W1003 03:00:20.262725 401706 metrics.cc:631] \"Unable to get power limit for GPU 0. Status:Success, value:0.000000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 03:00:20,649 - INFO - pytriton.triton: Infer function available as model: `/v2/models/AMPLIFY`\n",
      "2024-10-03 03:00:20,650 - INFO - pytriton.triton:   Status:         `GET  /v2/models/AMPLIFY/ready/`\n",
      "2024-10-03 03:00:20,651 - INFO - pytriton.triton:   Model config:   `GET  /v2/models/AMPLIFY/config/`\n",
      "2024-10-03 03:00:20,652 - INFO - pytriton.triton:   Inference:      `POST /v2/models/AMPLIFY/infer/`\n",
      "2024-10-03 03:00:20,653 - INFO - pytriton.triton: Read more about configuring and serving models in documentation: https://triton-inference-server.github.io/pytriton.\n",
      "2024-10-03 03:00:20,654 - INFO - pytriton.triton: (Press CTRL+C or use the command `kill -SIGINT 387772` to send a SIGINT signal and quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1003 03:00:20.633789 401706 model_lifecycle.cc:838] \"successfully loaded 'AMPLIFY'\"\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from pytriton.decorators import batch\n",
    "from pytriton.model_config import ModelConfig, Tensor\n",
    "from pytriton.triton import Triton, TritonConfig\n",
    "from pytriton.proxy.types import Request\n",
    "\n",
    "class AMPLIFYWrapper:\n",
    "    def __init__(self, model: amplify.inference.Predictor, device: str):\n",
    "        self._model = model\n",
    "        self._device = device\n",
    "\n",
    "    # @batch\n",
    "    def infer_func(self, requests: List[Request]) -> List[Dict[str, np.ndarray]]:\n",
    "         # unpack the request (single prompt in single item requests list)\n",
    "        request = {key: value for key, value in requests[0].items()}\n",
    "        sequences = request[\"sequences\"]\n",
    "        embeddings = [predictor.embed(x.decode(\"utf-8\")) for x in sequences]\n",
    "        max_length = max(embedding.shape[0] for embedding in embeddings)\n",
    "        embeddings = [np.pad(embedding, \n",
    "                            ((0, max_length - embedding.shape[0]), (0, 0)), \n",
    "                            mode='constant', \n",
    "                            constant_values=0) \n",
    "                     for embedding in embeddings]\n",
    "        return [{'embeddings': np.array(embeddings, dtype=np.float32)}]\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return [Tensor(name=\"sequences\", dtype=bytes, shape=(-1,)),]\n",
    "    \n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return [Tensor(name=\"embeddings\", dtype=np.float32, shape=(-1,-1,960)),]\n",
    "\n",
    "logger = logging.getLogger(\"examples.identity_python.server\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(name)s: %(message)s\")\n",
    "    \n",
    "amplify_deploy = AMPLIFYWrapper(predictor, \"cuda\")\n",
    "triton = Triton(config=TritonConfig(http_port=9999))\n",
    "triton.bind(model_name=\"AMPLIFY\",\n",
    "    infer_func=amplify_deploy.infer_func,\n",
    "    inputs=amplify_deploy.inputs,\n",
    "    outputs=amplify_deploy.outputs,\n",
    "    config=ModelConfig(batching=False, decoupled=False),\n",
    "    strict=True,)\n",
    "triton.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ceba3ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20, 960)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f75d22b7a60>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHQAAADTCAYAAADgWkTzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa20lEQVR4nO3dfYyV5d0n8N+ZgZlBZUZZkHHigKAVt7ZgFyqhVVdSwkuypqZNY9VNwBpsG+zGYGOltrykZonW8GgbVusfim1sS5OmNtuuBJbWajeoFZe19FFWfGgdxcGXVQbmeZiXc+79Q5g6dc4whzkz99y3n4+50p7ruufcv5G5OfjleikkSZIEAAAAAJlRk3YBAAAAAFRGoAMAAACQMQIdAAAAgIwR6AAAAABkjEAHAAAAIGMEOgAAAAAZI9ABAAAAyBiBDgAAAEDGCHQAAAAAMkagAwAAAJAxuQt0Nm/eHOedd140NDTE/Pnz49lnn027JGCY1q9fH4VCoV+76KKL0i4LOAVPPvlkXHXVVdHS0hKFQiEee+yxfuNJksTatWvjnHPOiQkTJsSiRYvi5ZdfTqdYoCIne75XrFjxoc/zpUuXplMsQA7kKtDZunVrrF69OtatWxfPP/98zJkzJ5YsWRJvvvlm2qUBw3TxxRfHG2+80df++Mc/pl0ScAo6Oztjzpw5sXnz5gHH77777vjBD34QDzzwQDzzzDNx+umnx5IlS+LYsWOjXClQqZM93xERS5cu7fd5/rOf/WwUKwTIl3FpF1BNmzZtipUrV8YNN9wQEREPPPBA/Pa3v42HHnoobr/99pSrA4Zj3Lhx0dzcnHYZwDAtW7Ysli1bNuBYkiRx7733xne+8534/Oc/HxERP/7xj2Pq1Knx2GOPxZe//OXRLBWo0GDP9wn19fU+zwGqJDczdLq7u2P37t2xaNGivr6amppYtGhR7Nq1K8XKgGp4+eWXo6WlJWbOnBnXX399vPrqq2mXBFTZgQMHor29vd9neVNTU8yfP99nOeTEE088EWeffXbMmjUrvv71r8c777yTdklATh07diw6OjoGbVmfAZybGTpvv/12FIvFmDp1ar/+qVOnxksvvZRSVUA1zJ8/P7Zs2RKzZs2KN954IzZs2BCXX3557N27NyZOnJh2eUCVtLe3R0QM+Fl+YgzIrqVLl8YXvvCFmDFjRrzyyivx7W9/O5YtWxa7du2K2tratMsDcuTYsWMxY/oZ0f5mcdDrmpub48CBA9HQ0DBKlVVXbgIdIL8+OH179uzZMX/+/Jg+fXr84he/iBtvvDHFygCAofrgsslPfvKTMXv27Dj//PPjiSeeiM997nMpVgbkTXd3d7S/WYz9z7VG48SBFyZ1HCnFBfPaoru7O7OBTm6WXE2ePDlqa2vj0KFD/foPHTpknS7kzJlnnhkXXnhh7N+/P+1SgCo68Xntsxw+GmbOnBmTJ0/2eQ6MmDMmFgZtWZebQKeuri7mzp0bO3fu7OsrlUqxc+fOWLBgQYqVAdV29OjReOWVV+Kcc85JuxSgimbMmBHNzc39Pss7OjrimWee8VkOOfTaa6/FO++84/McGDGlk/yTdblacrV69epYvnx5zJs3Ly699NK49957o7Ozs+/UKyCbvvnNb8ZVV10V06dPj4MHD8a6deuitrY2rr322rRLAyp09OjRfn8bf+DAgdizZ09MmjQppk2bFrfcckvceeed8bGPfSxmzJgR3/3ud6OlpSWuvvrq9IoGhmSw53vSpEmxYcOG+OIXvxjNzc3xyiuvxG233RYXXHBBLFmyJMWqgTzrSUrRk5Qfy7pcBTrXXHNNvPXWW7F27dpob2+PSy65JLZt2/ahzRWBbHnttdfi2muvjXfeeSemTJkSl112WTz99NMxZcqUtEsDKvTcc8/FwoUL+16vXr06IiKWL18eW7Zsidtuuy06Ozvjpptuivfeey8uu+yy2LZtW2bXtsNHyWDP9/333x8vvPBCPPLII/Hee+9FS0tLLF68OL73ve9FfX19WiUDOVeKJIoxcKJTKtOfJYUkSbL/XQAAAADE+0u2m5qa4pWXmmNimU2RjxwpxfkXtcfhw4ejsbFxlCusjlzN0AEAAACIiOhJkugpM4elXH+WCHQAAACA3CkOsuSqXH+WCHQAAACA3OlJYpBNkUe3lpEg0AEAAABypxSFKEah7FjWCXQAAACA3Ckl77dyY1kn0AEAAABypztqojsGPuWqe5RrGQkCHQAAACB3SkkhSkmZJVdl+rNk4Kgqw7q6umL9+vXR1dWVdilAlXm+Ib8835Bfnm8gLcXje+iUa1lXSJIcHL7+AR0dHdHU1BSHDx+OxsbGtMsBqsjzDfnl+Yb88nwDo+3E7zs7/zwtTp848DyWziOl+NwnX830702WXAEAAAC5M9hMnDzM0BHoAAAAALnTk9RGT1JbZqw4ytVU36gHOqVSKQ4ePBgTJ06MQqH6iVhHR0e//wXyw/MN+eX5hvzyfEN6kiSJI0eOREtLS9TU5G4L3ZMyQ6fKDh48GK2trSN+n9G4B5AOzzfkl+cb8svzDelpa2uLc889N+0yRl0xqYliMnCQVczBdsKjHuhMnDgxIiI+9Z/uiNrxDaN9+6r5f5/IR7p5xt/SrqA63p1TSruE4ZvYk3YFVbH4wpfSLmHYLjz9jbRLqIrdHdPTLqEqnv7TRWmXMGxn/nP2/wYoImLSj59Nu4Rhe+crl6ZdQlWMP5r9P4RGREz606G0Sxi2N/9jc9olVMW4f8v+z9SZ/5yPGUBnbsr+c8HY0dPZHf/96p/1/Xf4R01v1EZPDLzkqneUaxkJox7onFhmVTu+IcZlONCpachHoFNbl3YF1VEzIQeBzoSBf6PJmrozxqddwrBNOD0f24uNL+bjAa9pyO5nxQm1dfkIdMYVsv9819Zl/+cpIqK2Lvv/8R0RMa6mPu0Shi03P1PF7P9MjavNx7Ho40/Px+c3Y8tIbHeSBWboAAAAAGRMKWqiFAMHOqUQ6AAAAACMOd1JbYwrc8pVd/bzHIEOAAAAkD+lpCZKZZZclSy5AgAAABh7eqImusvM0Omx5AoAAABg7Bl8D53sH3Qk0AEAAAByZ/BTrgQ6AAAAAGNOzyCbIvfYQwcAAABg7ClGTRTLLK0q158lAh0AAAAgd0pJIUpJoexY1gl0AAAAgNzpTcZFTzJw7NGb/RVXAh0AAAAgf4pRiGIMPBOnXH+WCHQAAACA3OlJaqK27KbIpVGupvoEOgAAAEDulJKaKJU5nrxcf5YIdAAAAIDcKSY1USwT3JTrzxKBDgAAAJA7vUlt9JRZctVryRUAAADA2OPYcgAAAICMKUZNFKPMkqsy/VlySt/B5s2b47zzzouGhoaYP39+PPvss9WuCwAAAOCU9Sa1g7asqzjQ2bp1a6xevTrWrVsXzz//fMyZMyeWLFkSb7755kjUBwAAAFCxYlIYtGVdxYHOpk2bYuXKlXHDDTfExz/+8XjggQfitNNOi4ceemgk6gMAAACoWLFUG71lWrH0EZuh093dHbt3745Fixb9/Q1qamLRokWxa9euAb+mq6srOjo6+jUAAACAkVSMwqAt6yoKdN5+++0oFosxderUfv1Tp06N9vb2Ab9m48aN0dTU1NdaW1tPvVoAAACAISglfz/p6sMt7eqGb8S3dV6zZk0cPny4r7W1tY30LQEAAICPuGpvijzWDoiqKNCZPHly1NbWxqFDh/r1Hzp0KJqbmwf8mvr6+mhsbOzXAAAAAEZSNTdFHosHRFUU6NTV1cXcuXNj586dfX2lUil27twZCxYsqHpxAAAAAKeilNQM2ioxFg+IGlfpF6xevTqWL18e8+bNi0svvTTuvffe6OzsjBtuuGEk6gMAAACoWDFqordMcFM8Pr/lHw9uqq+vj/r6+n59Jw6IWrNmTV/fyQ6IGg0VBzrXXHNNvPXWW7F27dpob2+PSy65JLZt2/ahjZIBAAAA0nJiA+RyYxHxoYOb1q1bF+vXr+/XN9gBUS+99FL1Cq5QxYFORMTNN98cN998c7VrAQAAAKiK3lJtFEoDb37ce7y/ra2t316//zg7Zyw7pUAHAAAAYCwrRSFKUWaGzvH+oRzedCoHRI2GET+2HAAAAGC0nVhyVa4N1Vg9IMoMHQAAACB3eks1USgNPI+lt0x/OWPxgCiBDgAAAJA7Q9kUeajG4gFRAh0AAAAgd4pJIQrlji2vMNCJGHsHRAl0AAAAgNyp5gydsUigAwAAAOSOQAcAAAAgY4qDbIpcrHBT5LFIoAMAAADkTikKUYoyM3TK9GeJQAcAAADIHUuuAAAAADLGkisAAACAjEmSQiRlZuKU688SgQ4AAACQO6WkEMWSJVcAAAAAmVGKQhRsigwAAACQHZZcAQAAAGRMsVSIKLPkqtxSrCwR6AAAAAC5Y4YOAAAAQMYIdEZI72k1kdRl99z3+nez/4sfEdF7etoVVMe4w9n9WeozqZh2BVXxpx/+h7RLGLbffmZO2iVUxb+f9VraJVTHlK60Kxi2Jzf8KO0SquK079WlXcKw/ZeD49MuoSr+z/pPpV1CVSQdR9IuYdh6G1rSLoHjat7pSLuEqvjbvRemXUJV1PYkaZcwbOM7sv/n897eY2mXkCpLrgAAAAAyJknKz8RJsp85CnQAAACA/CklhSiUCXRKllwBAAAAjD320AEAAADImuR4KzeWcQIdAAAAIHeSUiFKZTY/TmyKDAAAADD2WHIFAAAAkDVJ4f1WbizjBDoAAABA7iSl91u5sawT6AAAAAC5Y8kVAAAAQMYkSaHs5scCHQAAAICxyLHlAAAAAFlTON7KjWWbQAcAAADIn9LxVm4s4wQ6AAAAQP44thwAAAAgW5Lk/VZuLOsEOgAAAED+lArvt3JjGSfQAQAAAHKnkLzfyo1lnUAHAAAAyB8zdAAAAAAyJjneyo1lnEAHAAAAyJ+cBzo1lX7Bk08+GVdddVW0tLREoVCIxx57bATKAgAAABiGE0uuyrWMqzjQ6ezsjDlz5sTmzZtHoh4AAACAYTuxKXK5lnUVL7latmxZLFu2bCRqAQAAAKgOS66Gp6urKzo6Ovo1AAAAgJFUiEFm6IzQPf/617/GjTfeGDNmzIgJEybE+eefH+vWrYvu7u5+173wwgtx+eWXR0NDQ7S2tsbdd99d8b1GfFPkjRs3xoYNG0b6NgAAAAB/lxTeb+XGRsBLL70UpVIpfvSjH8UFF1wQe/fujZUrV0ZnZ2fcc889ERHR0dERixcvjkWLFsUDDzwQf/7zn+MrX/lKnHnmmXHTTTcN+V4jHuisWbMmVq9e3fe6o6MjWltbR/q2AAAAwEdZ6XgrNzYCli5dGkuXLu17PXPmzNi3b1/cf//9fYHOo48+Gt3d3fHQQw9FXV1dXHzxxbFnz57YtGlTRYHOiC+5qq+vj8bGxn4NAAAAYCQNZVPkf9wipqurq+p1HD58OCZNmtT3eteuXXHFFVdEXV1dX9+SJUti37598e677w75fUc80AEAAAAYdclJWkS0trZGU1NTX9u4cWNVS9i/f3/88Ic/jK9+9at9fe3t7TF16tR+15143d7ePuT3rnjJ1dGjR2P//v19rw8cOBB79uyJSZMmxbRp0yp9OwAAAICqK5Teb+XGIiLa2tr6rSSqr68f8Prbb7897rrrrkHv9+KLL8ZFF13U9/r111+PpUuXxpe+9KVYuXJlZcUPQcWBznPPPRcLFy7se31if5zly5fHli1bqlYYAAAAwCkbwqbIQ90a5tZbb40VK1YMes3MmTP7/v/Bgwdj4cKF8ZnPfCYefPDBftc1NzfHoUOH+vWdeN3c3HzSWk6oONC58sorI0lycGA7AAAAkF8fWFo14FgFpkyZElOmTBnSta+//nosXLgw5s6dGw8//HDU1PTf7WbBggVxxx13RE9PT4wfPz4iInbs2BGzZs2Ks846a8g12UMHAAAAyJ0TS67KtZHw+uuvx5VXXhnTpk2Le+65J956661ob2/vtzfOddddF3V1dXHjjTfGX/7yl9i6dWvcd999/U4IH4oRP7YcAAAAYNR94DSrgcZGwo4dO2L//v2xf//+OPfcc/vf8vhqp6ampti+fXusWrUq5s6dG5MnT461a9dWdGR5hEAHAAAAyKPS8VZubASsWLHipHvtRETMnj07nnrqqWHdS6ADAAAA5E5hkBk6ZWfuZIg9dAAAAAAyxgwdAAAAIHcG2/x4pDZFHk0CHQAAACCfcrC0qhyBDgAAAJA/SZQPdHIQ9Ah0AAAAgNyx5AoAAAAgY/J+ypVABwAAAMif0vFWbizjBDoAAABA7pihAwAAAJA1NkUGAAAAyBabIgMAAABkjRk6AAAAANliDx0AAACArHHK1ciY8GZvjBvfm9bth+3YvxufdglVMeGtHPwUR0RpXE3aJQxb03MNaZdQFaf/rSPtEobt6OfTrqA67pj+m7RLqIqPXfBvaZcwbJ/6X19Pu4SqaNh1RtolDFvXZ46kXUJVXP7dv6RdQlX87wdnp13CsE16sSvtEqqiecO/pF3C8K1Iu4DqaIr/m3YJ5EhPZ3fEH9KuIj2F463cWNaZoQMAAADkjk2RAQAAALLGpsgAAAAAGZSD4KYcgQ4AAACQO5ZcAQAAAGSMY8sBAAAAssYeOgAAAADZYskVAAAAQNaYoQMAAACQLWboAAAAAGRMIUmikAw8Fadcf5YIdAAAAID8seQKAAAAIFssuQIAAADImELyfis3lnUCHQAAACB/LLkCAAAAyBZLrgAAAAAyKA9Lq8oR6AAAAAC5UyglUSiVOba8TH+WCHQAAACA/LGHDgAAAEC22EMHAAAAIGPyHujUVHLxxo0b49Of/nRMnDgxzj777Lj66qtj3759I1UbAAAAwKlJksFbxlUU6PzhD3+IVatWxdNPPx07duyInp6eWLx4cXR2do5UfQAAAAAVKySDt6yrKNDZtm1brFixIi6++OKYM2dObNmyJV599dXYvXv3SNUHAAAAULETS67KtZHW1dUVl1xySRQKhdizZ0+/sRdeeCEuv/zyaGhoiNbW1rj77rsrfv+KAp1/dPjw4YiImDRp0nDeBgAAAKC6Ul5yddttt0VLS8uH+js6OmLx4sUxffr02L17d3z/+9+P9evXx4MPPljR+5/ypsilUiluueWW+OxnPxuf+MQnyl7X1dUVXV1d/QoHAAAAGElpbor8+OOPx/bt2+OXv/xlPP744/3GHn300eju7o6HHnoo6urq4uKLL449e/bEpk2b4qabbhryPU55hs6qVati79698fOf/3zQ6zZu3BhNTU19rbW19VRvCQAAADAkQ9lDp6Ojo1/74ISUU3Xo0KFYuXJl/OQnP4nTTjvtQ+O7du2KK664Iurq6vr6lixZEvv27Yt33313yPc5pUDn5ptvjt/85jfx+9//Ps4999xBr12zZk0cPny4r7W1tZ3KLQEAAACGrpQM3iKitbW13ySUjRs3DuuWSZLEihUr4mtf+1rMmzdvwGva29tj6tSp/fpOvG5vbx/yvSpacpUkSXzjG9+IX/3qV/HEE0/EjBkzTvo19fX1UV9fX8ltAAAAAIalkAyy5Or4DJ22trZobGzs6y+XX9x+++1x1113DXq/F198MbZv3x5HjhyJNWvWnFLNlago0Fm1alX89Kc/jV//+tcxceLEvuSoqakpJkyYMCIFAgAAAFRssM2Pj/c3Njb2C3TKufXWW2PFihWDXjNz5sz43e9+F7t27fpQMDRv3ry4/vrr45FHHonm5uY4dOhQv/ETr5ubm09aywkVBTr3339/RERceeWV/foffvjhk35jAAAAAKOlmpsiT5kyJaZMmXLS637wgx/EnXfe2ff64MGDsWTJkti6dWvMnz8/IiIWLFgQd9xxR/T09MT48eMjImLHjh0xa9asOOuss4ZcU8VLrgAAAADGukKSRKFMjlGuf7imTZvW7/UZZ5wRERHnn39+3x7E1113XWzYsCFuvPHG+Na3vhV79+6N++67L/7pn/6ponud8rHlAAAAAGNW6XgrN5aSpqam2L59e6xatSrmzp0bkydPjrVr11Z0ZHmEQAcAAADIoUIpiUKpzAydMv3Vdt555w242mn27Nnx1FNPDeu9BToAAABA/gxhU+QsE+gAAAAAuVNI/n48+UBjWSfQAQAAAHKnUEyiUCa5KRSzn+gIdAAAAID8seQKAAAAIFvGwqbII0mgAwAAAOSPGToAAAAAGZNERGmQsYwT6AAAAAC5UyglUSgMnOhYcgUAAAAwFllyBQAAAJAxpYgoDDKWcQIdAAAAIHcKpdIgS66yn+gIdAAAAID8seQKAAAAIGOKSZQ9zqoo0AEAAAAYcwpJEoUyM3HK9WeJQAcAAADIH0uuqis5/i+tt/fYaN+6qopdxbRLqIpiT/Y3goqIKHbVpF3CsPX25ONnqrfYlXYJw1b817QrqI7OI/l4vo+Mz/73UfrXbH/mnVDsyv7fAxVz8mvRfbQ77RKqotid/V+P3t58/Fr0dObj+wD6O/FsJzkIL05JsRRlj7MqZv/PmKP+J7MjR45ERMSffr9xtG9dXTvSLgAYMf857QKqY1naBfAB/zXtAjjhv6VdQHX8S9oFkD+/T7sAYCQdOXIkmpqa0i4jBYPM0Cm3t06GjHqg09LSEm1tbTFx4sQoFModCH/qOjo6orW1Ndra2qKxsbHq7w+kx/MN+eX5hvzyfEN6kiSJI0eOREtLS9qlpMOSq+qqqamJc889d8Tv09jY6AMDcsrzDfnl+Yb88nxDOj6aM3OOKxYjkjJbW5Syv+VF9hfDAwAAAPwjM3QAAAAAMqZYikjKbH5csinymFNfXx/r1q2L+vr6tEsBqszzDfnl+Yb88nwDqUlikBk6o1rJiCgkH9nzywAAAIC86ejoiKampljUfFOMq6kb8JreUnf8z/YH4/Dhw5nd3yt3M3QAAAAAbIoMAAAAkDU2RQYAAADImFISZTfLKQl0AAAAAMacpFSMpMySq3L9WSLQAQAAAPInGWSGjiVXAAAAAGNQsRhRKDMTxwwdAAAAgLEnKZUiKZQGHksG7s8SgQ4AAACQP5ZcAQAAAGRMsTTIkiszdAAAAADGnKSURFIYeCZOYoYOAAAAwBiUlCKizEwcM3QAAAAAxp6e4rFIYuAlV73RM8rVVJ9ABwAAAMiNurq6aG5ujj+2/49Br2tubo66urpRqqr6CkkeFo4BAAAAHHfs2LHo7u4e9Jq6urpoaGgYpYqqT6ADAAAAkDE1aRcAAAAAQGUEOgAAAAAZI9ABAAAAyBiBDgAAAEDGCHQAAAAAMkagAwAAAJAxAh0AAACAjPn/kgyiziw1ngQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1600x240 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sequences = [\"AACGGEVWVTDEAAAAA\",\n",
    "             \"AAAAACGGGVWWTDEAAAAA\",\n",
    "             \"AAAADGGVWVTECDA\",]\n",
    "\n",
    "url = \"http://localhost:9999/v2/models/AMPLIFY/generate_stream\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "response = requests.post(url, headers=headers, data=json.dumps({\"sequences\": sequences}))\n",
    "if response.status_code == 200:\n",
    "    response_dict = json.loads(response.text.split(\"data:\")[1])\n",
    "    embeddings = np.array(response_dict[\"embeddings\"]).reshape(len(sequences), -1, 960)\n",
    "\n",
    "    print(embeddings.shape)\n",
    "    plt.matshow(embeddings[..., 0])\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82b1c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 03:00:15,078 - WARNING - pytriton.server.triton_server: Triton Inference Server exited with failure. Please wait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 03:00:15,103 - INFO - pytriton.proxy.inference: Closing Inference Handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1003 03:00:15.183640 401025 pb_stub.cc:2049]  Non-graceful termination detected. \n",
      "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
