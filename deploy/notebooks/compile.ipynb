{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cede9f0-70c7-45e3-9a3b-7c75f6a984e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-01 16:41:08,237] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[NeMo W 2024-10-01 16:41:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:289: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:300: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:392: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:432: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:10 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:10 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-10-01 16:41:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "[NeMo I 2024-10-01 16:41:13 convert_checkpoint_to_nemo:135] loading checkpoint ../../../data/models/AMPLIFY/AMPLIFY_350M/pytorch_model.pt\n",
      "amplify_config: {'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 2048, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 3, 'pad_token_id': 0, 'eos_token_id': 4, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', '_commit_hash': None, '_attn_implementation_internal': None, 'transformers_version': None, '_name_': 'AMPLIFY', 'pre_activation_layer_norm': True, 'rope_theta': 10000.0, 'vocab_path': '/workspace/ynashed-workspace/AMPLIFY/conf/tokenizer/amplify_vocab.txt', 'mask_token_id': 2, 'unk_token_id': 1, 'other_special_token_ids': None, 'hidden_size': 960, 'num_hidden_layers': 32, 'num_attention_heads': 15, 'intermediate_size': 3840, 'dropout_prob': 0, 'embedding_init_range': 0.02, 'decoder_init_range': 0.02, 'rms_norm': True, 'norm_eps': 1e-05, 'hidden_act': 'SwiGLU', 'layer_norm_after_embedding': False, 'layer_norm_before_last_layer': True, 'vocab_size': 27, 'ffn_bias': False, 'att_bias': False}\n",
      "named parameters:\n",
      "- encoder.weight\n",
      "- transformer_encoder.0.q.weight\n",
      "- transformer_encoder.0.k.weight\n",
      "- transformer_encoder.0.v.weight\n",
      "- transformer_encoder.0.wo.weight\n",
      "- transformer_encoder.0.ffn.w12.weight\n",
      "- transformer_encoder.0.ffn.w3.weight\n",
      "- transformer_encoder.0.attention_norm.weight\n",
      "- transformer_encoder.0.ffn_norm.weight\n",
      "- transformer_encoder.1.q.weight\n",
      "- transformer_encoder.1.k.weight\n",
      "- transformer_encoder.1.v.weight\n",
      "- transformer_encoder.1.wo.weight\n",
      "- transformer_encoder.1.ffn.w12.weight\n",
      "- transformer_encoder.1.ffn.w3.weight\n",
      "- transformer_encoder.1.attention_norm.weight\n",
      "- transformer_encoder.1.ffn_norm.weight\n",
      "- transformer_encoder.2.q.weight\n",
      "- transformer_encoder.2.k.weight\n",
      "- transformer_encoder.2.v.weight\n",
      "- transformer_encoder.2.wo.weight\n",
      "- transformer_encoder.2.ffn.w12.weight\n",
      "- transformer_encoder.2.ffn.w3.weight\n",
      "- transformer_encoder.2.attention_norm.weight\n",
      "- transformer_encoder.2.ffn_norm.weight\n",
      "- transformer_encoder.3.q.weight\n",
      "- transformer_encoder.3.k.weight\n",
      "- transformer_encoder.3.v.weight\n",
      "- transformer_encoder.3.wo.weight\n",
      "- transformer_encoder.3.ffn.w12.weight\n",
      "- transformer_encoder.3.ffn.w3.weight\n",
      "- transformer_encoder.3.attention_norm.weight\n",
      "- transformer_encoder.3.ffn_norm.weight\n",
      "- transformer_encoder.4.q.weight\n",
      "- transformer_encoder.4.k.weight\n",
      "- transformer_encoder.4.v.weight\n",
      "- transformer_encoder.4.wo.weight\n",
      "- transformer_encoder.4.ffn.w12.weight\n",
      "- transformer_encoder.4.ffn.w3.weight\n",
      "- transformer_encoder.4.attention_norm.weight\n",
      "- transformer_encoder.4.ffn_norm.weight\n",
      "- transformer_encoder.5.q.weight\n",
      "- transformer_encoder.5.k.weight\n",
      "- transformer_encoder.5.v.weight\n",
      "- transformer_encoder.5.wo.weight\n",
      "- transformer_encoder.5.ffn.w12.weight\n",
      "- transformer_encoder.5.ffn.w3.weight\n",
      "- transformer_encoder.5.attention_norm.weight\n",
      "- transformer_encoder.5.ffn_norm.weight\n",
      "- transformer_encoder.6.q.weight\n",
      "- transformer_encoder.6.k.weight\n",
      "- transformer_encoder.6.v.weight\n",
      "- transformer_encoder.6.wo.weight\n",
      "- transformer_encoder.6.ffn.w12.weight\n",
      "- transformer_encoder.6.ffn.w3.weight\n",
      "- transformer_encoder.6.attention_norm.weight\n",
      "- transformer_encoder.6.ffn_norm.weight\n",
      "- transformer_encoder.7.q.weight\n",
      "- transformer_encoder.7.k.weight\n",
      "- transformer_encoder.7.v.weight\n",
      "- transformer_encoder.7.wo.weight\n",
      "- transformer_encoder.7.ffn.w12.weight\n",
      "- transformer_encoder.7.ffn.w3.weight\n",
      "- transformer_encoder.7.attention_norm.weight\n",
      "- transformer_encoder.7.ffn_norm.weight\n",
      "- transformer_encoder.8.q.weight\n",
      "- transformer_encoder.8.k.weight\n",
      "- transformer_encoder.8.v.weight\n",
      "- transformer_encoder.8.wo.weight\n",
      "- transformer_encoder.8.ffn.w12.weight\n",
      "- transformer_encoder.8.ffn.w3.weight\n",
      "- transformer_encoder.8.attention_norm.weight\n",
      "- transformer_encoder.8.ffn_norm.weight\n",
      "- transformer_encoder.9.q.weight\n",
      "- transformer_encoder.9.k.weight\n",
      "- transformer_encoder.9.v.weight\n",
      "- transformer_encoder.9.wo.weight\n",
      "- transformer_encoder.9.ffn.w12.weight\n",
      "- transformer_encoder.9.ffn.w3.weight\n",
      "- transformer_encoder.9.attention_norm.weight\n",
      "- transformer_encoder.9.ffn_norm.weight\n",
      "- transformer_encoder.10.q.weight\n",
      "- transformer_encoder.10.k.weight\n",
      "- transformer_encoder.10.v.weight\n",
      "- transformer_encoder.10.wo.weight\n",
      "- transformer_encoder.10.ffn.w12.weight\n",
      "- transformer_encoder.10.ffn.w3.weight\n",
      "- transformer_encoder.10.attention_norm.weight\n",
      "- transformer_encoder.10.ffn_norm.weight\n",
      "- transformer_encoder.11.q.weight\n",
      "- transformer_encoder.11.k.weight\n",
      "- transformer_encoder.11.v.weight\n",
      "- transformer_encoder.11.wo.weight\n",
      "- transformer_encoder.11.ffn.w12.weight\n",
      "- transformer_encoder.11.ffn.w3.weight\n",
      "- transformer_encoder.11.attention_norm.weight\n",
      "- transformer_encoder.11.ffn_norm.weight\n",
      "- transformer_encoder.12.q.weight\n",
      "- transformer_encoder.12.k.weight\n",
      "- transformer_encoder.12.v.weight\n",
      "- transformer_encoder.12.wo.weight\n",
      "- transformer_encoder.12.ffn.w12.weight\n",
      "- transformer_encoder.12.ffn.w3.weight\n",
      "- transformer_encoder.12.attention_norm.weight\n",
      "- transformer_encoder.12.ffn_norm.weight\n",
      "- transformer_encoder.13.q.weight\n",
      "- transformer_encoder.13.k.weight\n",
      "- transformer_encoder.13.v.weight\n",
      "- transformer_encoder.13.wo.weight\n",
      "- transformer_encoder.13.ffn.w12.weight\n",
      "- transformer_encoder.13.ffn.w3.weight\n",
      "- transformer_encoder.13.attention_norm.weight\n",
      "- transformer_encoder.13.ffn_norm.weight\n",
      "- transformer_encoder.14.q.weight\n",
      "- transformer_encoder.14.k.weight\n",
      "- transformer_encoder.14.v.weight\n",
      "- transformer_encoder.14.wo.weight\n",
      "- transformer_encoder.14.ffn.w12.weight\n",
      "- transformer_encoder.14.ffn.w3.weight\n",
      "- transformer_encoder.14.attention_norm.weight\n",
      "- transformer_encoder.14.ffn_norm.weight\n",
      "- transformer_encoder.15.q.weight\n",
      "- transformer_encoder.15.k.weight\n",
      "- transformer_encoder.15.v.weight\n",
      "- transformer_encoder.15.wo.weight\n",
      "- transformer_encoder.15.ffn.w12.weight\n",
      "- transformer_encoder.15.ffn.w3.weight\n",
      "- transformer_encoder.15.attention_norm.weight\n",
      "- transformer_encoder.15.ffn_norm.weight\n",
      "- transformer_encoder.16.q.weight\n",
      "- transformer_encoder.16.k.weight\n",
      "- transformer_encoder.16.v.weight\n",
      "- transformer_encoder.16.wo.weight\n",
      "- transformer_encoder.16.ffn.w12.weight\n",
      "- transformer_encoder.16.ffn.w3.weight\n",
      "- transformer_encoder.16.attention_norm.weight\n",
      "- transformer_encoder.16.ffn_norm.weight\n",
      "- transformer_encoder.17.q.weight\n",
      "- transformer_encoder.17.k.weight\n",
      "- transformer_encoder.17.v.weight\n",
      "- transformer_encoder.17.wo.weight\n",
      "- transformer_encoder.17.ffn.w12.weight\n",
      "- transformer_encoder.17.ffn.w3.weight\n",
      "- transformer_encoder.17.attention_norm.weight\n",
      "- transformer_encoder.17.ffn_norm.weight\n",
      "- transformer_encoder.18.q.weight\n",
      "- transformer_encoder.18.k.weight\n",
      "- transformer_encoder.18.v.weight\n",
      "- transformer_encoder.18.wo.weight\n",
      "- transformer_encoder.18.ffn.w12.weight\n",
      "- transformer_encoder.18.ffn.w3.weight\n",
      "- transformer_encoder.18.attention_norm.weight\n",
      "- transformer_encoder.18.ffn_norm.weight\n",
      "- transformer_encoder.19.q.weight\n",
      "- transformer_encoder.19.k.weight\n",
      "- transformer_encoder.19.v.weight\n",
      "- transformer_encoder.19.wo.weight\n",
      "- transformer_encoder.19.ffn.w12.weight\n",
      "- transformer_encoder.19.ffn.w3.weight\n",
      "- transformer_encoder.19.attention_norm.weight\n",
      "- transformer_encoder.19.ffn_norm.weight\n",
      "- transformer_encoder.20.q.weight\n",
      "- transformer_encoder.20.k.weight\n",
      "- transformer_encoder.20.v.weight\n",
      "- transformer_encoder.20.wo.weight\n",
      "- transformer_encoder.20.ffn.w12.weight\n",
      "- transformer_encoder.20.ffn.w3.weight\n",
      "- transformer_encoder.20.attention_norm.weight\n",
      "- transformer_encoder.20.ffn_norm.weight\n",
      "- transformer_encoder.21.q.weight\n",
      "- transformer_encoder.21.k.weight\n",
      "- transformer_encoder.21.v.weight\n",
      "- transformer_encoder.21.wo.weight\n",
      "- transformer_encoder.21.ffn.w12.weight\n",
      "- transformer_encoder.21.ffn.w3.weight\n",
      "- transformer_encoder.21.attention_norm.weight\n",
      "- transformer_encoder.21.ffn_norm.weight\n",
      "- transformer_encoder.22.q.weight\n",
      "- transformer_encoder.22.k.weight\n",
      "- transformer_encoder.22.v.weight\n",
      "- transformer_encoder.22.wo.weight\n",
      "- transformer_encoder.22.ffn.w12.weight\n",
      "- transformer_encoder.22.ffn.w3.weight\n",
      "- transformer_encoder.22.attention_norm.weight\n",
      "- transformer_encoder.22.ffn_norm.weight\n",
      "- transformer_encoder.23.q.weight\n",
      "- transformer_encoder.23.k.weight\n",
      "- transformer_encoder.23.v.weight\n",
      "- transformer_encoder.23.wo.weight\n",
      "- transformer_encoder.23.ffn.w12.weight\n",
      "- transformer_encoder.23.ffn.w3.weight\n",
      "- transformer_encoder.23.attention_norm.weight\n",
      "- transformer_encoder.23.ffn_norm.weight\n",
      "- transformer_encoder.24.q.weight\n",
      "- transformer_encoder.24.k.weight\n",
      "- transformer_encoder.24.v.weight\n",
      "- transformer_encoder.24.wo.weight\n",
      "- transformer_encoder.24.ffn.w12.weight\n",
      "- transformer_encoder.24.ffn.w3.weight\n",
      "- transformer_encoder.24.attention_norm.weight\n",
      "- transformer_encoder.24.ffn_norm.weight\n",
      "- transformer_encoder.25.q.weight\n",
      "- transformer_encoder.25.k.weight\n",
      "- transformer_encoder.25.v.weight\n",
      "- transformer_encoder.25.wo.weight\n",
      "- transformer_encoder.25.ffn.w12.weight\n",
      "- transformer_encoder.25.ffn.w3.weight\n",
      "- transformer_encoder.25.attention_norm.weight\n",
      "- transformer_encoder.25.ffn_norm.weight\n",
      "- transformer_encoder.26.q.weight\n",
      "- transformer_encoder.26.k.weight\n",
      "- transformer_encoder.26.v.weight\n",
      "- transformer_encoder.26.wo.weight\n",
      "- transformer_encoder.26.ffn.w12.weight\n",
      "- transformer_encoder.26.ffn.w3.weight\n",
      "- transformer_encoder.26.attention_norm.weight\n",
      "- transformer_encoder.26.ffn_norm.weight\n",
      "- transformer_encoder.27.q.weight\n",
      "- transformer_encoder.27.k.weight\n",
      "- transformer_encoder.27.v.weight\n",
      "- transformer_encoder.27.wo.weight\n",
      "- transformer_encoder.27.ffn.w12.weight\n",
      "- transformer_encoder.27.ffn.w3.weight\n",
      "- transformer_encoder.27.attention_norm.weight\n",
      "- transformer_encoder.27.ffn_norm.weight\n",
      "- transformer_encoder.28.q.weight\n",
      "- transformer_encoder.28.k.weight\n",
      "- transformer_encoder.28.v.weight\n",
      "- transformer_encoder.28.wo.weight\n",
      "- transformer_encoder.28.ffn.w12.weight\n",
      "- transformer_encoder.28.ffn.w3.weight\n",
      "- transformer_encoder.28.attention_norm.weight\n",
      "- transformer_encoder.28.ffn_norm.weight\n",
      "- transformer_encoder.29.q.weight\n",
      "- transformer_encoder.29.k.weight\n",
      "- transformer_encoder.29.v.weight\n",
      "- transformer_encoder.29.wo.weight\n",
      "- transformer_encoder.29.ffn.w12.weight\n",
      "- transformer_encoder.29.ffn.w3.weight\n",
      "- transformer_encoder.29.attention_norm.weight\n",
      "- transformer_encoder.29.ffn_norm.weight\n",
      "- transformer_encoder.30.q.weight\n",
      "- transformer_encoder.30.k.weight\n",
      "- transformer_encoder.30.v.weight\n",
      "- transformer_encoder.30.wo.weight\n",
      "- transformer_encoder.30.ffn.w12.weight\n",
      "- transformer_encoder.30.ffn.w3.weight\n",
      "- transformer_encoder.30.attention_norm.weight\n",
      "- transformer_encoder.30.ffn_norm.weight\n",
      "- transformer_encoder.31.q.weight\n",
      "- transformer_encoder.31.k.weight\n",
      "- transformer_encoder.31.v.weight\n",
      "- transformer_encoder.31.wo.weight\n",
      "- transformer_encoder.31.ffn.w12.weight\n",
      "- transformer_encoder.31.ffn.w3.weight\n",
      "- transformer_encoder.31.attention_norm.weight\n",
      "- transformer_encoder.31.ffn_norm.weight\n",
      "- layer_norm_2.weight\n",
      "- decoder.weight\n",
      "- decoder.bias\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/ynashed-workspace/AMPLIFY/deploy/notebooks/../scripts/convert_checkpoint_to_nemo.py\", line 342, in <module>\n",
      "    convert(args)\n",
      "  File \"/workspace/ynashed-workspace/AMPLIFY/deploy/notebooks/../scripts/convert_checkpoint_to_nemo.py\", line 143, in convert\n",
      "    nemo_config = load_config(args, amplify_config)\n",
      "  File \"/workspace/ynashed-workspace/AMPLIFY/deploy/notebooks/../scripts/convert_checkpoint_to_nemo.py\", line 95, in load_config\n",
      "    nemo_config.init_method_std = amplify_config['initializer_range']\n",
      "KeyError: 'initializer_range'\n",
      "[NeMo W 2024-10-01 16:41:17 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmps0mhg16m'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import amplify\n",
    "import os\n",
    "\n",
    "# load the model\n",
    "MODEL = \"AMPLIFY_350M\"\n",
    "path = f\"../../../data/models/AMPLIFY/{MODEL}/\"\n",
    "config_path = os.path.join(path, \"config.yaml\")\n",
    "checkpoint_file = os.path.join(path, \"pytorch_model.pt\")\n",
    "\n",
    "# model, tokenizer = amplify.AMPLIFY.load(checkpoint_file, config_path)\n",
    "# model = model.eval()\n",
    "!python ../scripts/convert_checkpoint_to_nemo.py \\\n",
    "        --input_name_or_path {checkpoint_file} \\\n",
    "        --output_path . \\\n",
    "        --config_name_or_path {config_path}\\\n",
    "        --hparams_file ../conf/megatron_amplify_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eded395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/models/AMPLIFY/AMPLIFY_350M/config.yaml\n"
     ]
    }
   ],
   "source": [
    "print(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfaa045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 2048, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 3, 'pad_token_id': 0, 'eos_token_id': 4, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', '_commit_hash': None, '_attn_implementation_internal': None, 'transformers_version': None, '_name_': 'AMPLIFY', 'pre_activation_layer_norm': True, 'vocab_path': '/workspace/ynashed-workspace/AMPLIFY/conf/tokenizer/amplify_vocab.txt', 'mask_token_id': 2, 'unk_token_id': 1, 'other_special_token_ids': None, 'hidden_size': 960, 'num_hidden_layers': 32, 'num_attention_heads': 15, 'intermediate_size': 3840, 'dropout_prob': 0, 'embedding_init_range': 0.02, 'decoder_init_range': 0.02, 'rms_norm': True, 'norm_eps': 1e-05, 'hidden_act': 'SwiGLU', 'layer_norm_after_embedding': False, 'layer_norm_before_last_layer': True, 'vocab_size': 27, 'ffn_bias': False, 'att_bias': False}\n"
     ]
    }
   ],
   "source": [
    "hf_config = vars(model.config)\n",
    "print(hf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed33f6a-7056-4c32-81cb-fa3c2b9847e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4713: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/workspace/AMPLIFY/src/amplify/model/rotary.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
      "/workspace/xformers/xformers/ops/fmha/common.py:219: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.query.shape == (B, Mq, H, K)\n",
      "/workspace/xformers/xformers/ops/fmha/common.py:220: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and self.key.shape == (B, Mkv, H, key_embed_dim)\n",
      "/workspace/xformers/xformers/ops/fmha/common.py:221: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and self.value.shape == (B, Mkv, H, Kv)\n",
      "/workspace/xformers/xformers/ops/fmha/common.py:336: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not cls.SUPPORTS_DIFFERENT_VALUE_EMBED and K != Kv:\n",
      "/workspace/xformers/xformers/ops/fmha/common.py:338: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max(K, Kv) > cls.SUPPORTED_MAX_K:\n",
      "/workspace/xformers/xformers/ops/fmha/common.py:342: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min(K, Kv) < cls.SUPPORTED_MIN_K:\n",
      "/workspace/xformers/xformers/ops/fmha/common.py:485: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape[-1] % alignment != 0:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unsupported output type: int, from operator: xformers::efficient_attention_forward_cutlass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAAAACGGGVWWTDEAAAAA\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m      4\u001b[0m                              random_truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 5\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:999\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    993\u001b[0m     check_if_torch_exportable,\n\u001b[1;32m    994\u001b[0m     log_torch_jit_trace_exportability,\n\u001b[1;32m    995\u001b[0m     log_torchscript_usage,\n\u001b[1;32m    996\u001b[0m )\n\u001b[1;32m    998\u001b[0m log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 999\u001b[0m traced_func \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_export\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TS2EPConverter\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:694\u001b[0m, in \u001b[0;36m_trace_impl\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m ):\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:1274\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1273\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1552\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1564\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1542\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1542\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/workspace/AMPLIFY/src/amplify/model/amplify.py:250\u001b[0m, in \u001b[0;36mAMPLIFY.forward\u001b[0;34m(self, src, pad_mask, output_hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Transformer encoder\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder:\n\u001b[0;32m--> 250\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    252\u001b[0m         hidden_states\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1552\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1564\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1542\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1542\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/workspace/AMPLIFY/src/amplify/model/amplify.py:131\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x, pad_mask, freqs_cis, output_attentions)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, pad_mask: torch\u001b[38;5;241m.\u001b[39mTensor, freqs_cis: torch\u001b[38;5;241m.\u001b[39mTensor, output_attentions: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 131\u001b[0m     attn, contact \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_att_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn\n\u001b[1;32m    133\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(x))\n",
      "File \u001b[0;32m/workspace/AMPLIFY/src/amplify/model/amplify.py:146\u001b[0m, in \u001b[0;36mEncoderBlock._att_block\u001b[0;34m(self, x, pad_mask, freqs_cis, output_attentions)\u001b[0m\n\u001b[1;32m    143\u001b[0m xv \u001b[38;5;241m=\u001b[39m xv\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head)\n\u001b[1;32m    144\u001b[0m xq, xk \u001b[38;5;241m=\u001b[39m apply_rotary_emb(xq, xk, freqs_cis)\n\u001b[0;32m--> 146\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mmemory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m _attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/workspace/xformers/xformers/ops/fmha/__init__.py:301\u001b[0m, in \u001b[0;36mmemory_efficient_attention\u001b[0;34m(query, key, value, attn_bias, p, scale, op, output_dtype)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmemory_efficient_attention\u001b[39m(\n\u001b[1;32m    190\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    191\u001b[0m     key: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     output_dtype: Optional[torch\u001b[38;5;241m.\u001b[39mdtype] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implements the memory-efficient attention mechanism following\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    `\"Self-Attention Does Not Need O(n^2) Memory\" <http://arxiv.org/abs/2112.05682>`_.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/xformers/xformers/ops/fmha/__init__.py:470\u001b[0m, in \u001b[0;36m_memory_efficient_attention\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    468\u001b[0m op_fw \u001b[38;5;241m=\u001b[39m _serialize_op(op[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    469\u001b[0m op_bw \u001b[38;5;241m=\u001b[39m _serialize_op(op[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fMHA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_fw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_bw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(output_shape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:573\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/xformers/xformers/ops/fmha/__init__.py:84\u001b[0m, in \u001b[0;36m_fMHA.forward\u001b[0;34m(ctx, op_fw, op_bw, *args)\u001b[0m\n\u001b[1;32m     81\u001b[0m op_fw \u001b[38;5;241m=\u001b[39m _unserialize_op(op_fw)\n\u001b[1;32m     82\u001b[0m op_bw \u001b[38;5;241m=\u001b[39m _unserialize_op(op_bw)\n\u001b[0;32m---> 84\u001b[0m out, op_ctx \u001b[38;5;241m=\u001b[39m \u001b[43m_memory_efficient_attention_forward_requires_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_fw\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Saving attn_bias is a bit complicated, as the\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# torch part should go in `save_for_backward`\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp\u001b[38;5;241m.\u001b[39mattn_bias, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/workspace/xformers/xformers/ops/fmha/__init__.py:498\u001b[0m, in \u001b[0;36m_memory_efficient_attention_forward_requires_grad\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     _ensure_op_supports_or_raise(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_efficient_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m, op, inp)\n\u001b[0;32m--> 498\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(output_shape), out[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/workspace/xformers/xformers/ops/fmha/cutlass.py:218\u001b[0m, in \u001b[0;36mFwOp.apply\u001b[0;34m(cls, inp, needs_gradient)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported attn_bias type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_bmhk\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery has shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minp\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m ctx: Optional[Context] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/xformers/xformers/ops/fmha/cutlass.py:282\u001b[0m, in \u001b[0;36mFwOp.apply_bmhk\u001b[0;34m(cls, inp, needs_gradient)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported attn_bias type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m seqstart_k, seqstart_q, max_seqlen_q, max_seqlen_k \u001b[38;5;241m=\u001b[39m _get_seqlen_info(inp)\n\u001b[0;32m--> 282\u001b[0m out, lse, rng_seed, rng_offset, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOPERATOR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_tensor_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqstart_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqstart_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_log_sumexp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_mask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_custom_mask_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlen_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_seqinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqlen\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBlockDiagonalCausalWithOffsetPaddedKeysMask\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_window_size\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBlockDiagonalCausalLocalAttentionMask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBlockDiagonalCausalLocalAttentionFromBottomRightMask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[43mLowerTriangularFromBottomRightLocalAttentionMask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m ctx: Optional[Context] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_gradient:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py:1060\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unsupported output type: int, from operator: xformers::efficient_attention_forward_cutlass"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = tokenizer.encode(\"AAAAACGGGVWWTDEAAAAA\", max_tokens=2048,\n",
    "                             random_truncate=False).unsqueeze(0).cuda()\n",
    "traced_model = torch.jit.trace(model.cuda(), input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
